{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 483 Project 3 - Regularization, Cross-Validation, and Grid Search\n",
    "#### by: Josef Jankowski(josefj1519@csu.fullerton.edu) and William Timani (williamtimani@csu.fullerton.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and examine the Boston dataset’s features, target values, and description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "dataset_boston = datasets.load_boston()\n",
    "print(dataset_boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Save CRIM as the new target value t, and drop the column CRIM from X. Add the target value MEDV to X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     MEDV    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  PTRATIO  \\\n",
      "0    24.0  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0     15.3   \n",
      "1    21.6   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0     17.8   \n",
      "2    34.7   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0     17.8   \n",
      "3    33.4   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0     18.7   \n",
      "4    36.2   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0     18.7   \n",
      "..    ...   ...    ...   ...    ...    ...   ...     ...  ...    ...      ...   \n",
      "501  22.4   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0     21.0   \n",
      "502  20.6   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0     21.0   \n",
      "503  23.9   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0     21.0   \n",
      "504  22.0   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0     21.0   \n",
      "505  11.9   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0     21.0   \n",
      "\n",
      "          B  LSTAT  \n",
      "0    396.90   4.98  \n",
      "1    396.90   9.14  \n",
      "2    392.83   4.03  \n",
      "3    394.63   2.94  \n",
      "4    396.90   5.33  \n",
      "..      ...    ...  \n",
      "501  391.99   9.67  \n",
      "502  396.90   9.08  \n",
      "503  396.90   5.64  \n",
      "504  393.45   6.48  \n",
      "505  396.90   7.88  \n",
      "\n",
      "[506 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Independent variables (i.e. features)\n",
    "df_boston_features = pd.DataFrame(data=dataset_boston.data, columns=dataset_boston.feature_names)\n",
    "df_boston_features.insert(0, 'MEDV', dataset_boston.target)\n",
    "df_boston_target = pd.DataFrame(data=df_boston_features['CRIM'], columns=['CRIM'])\n",
    "df_boston_features = df_boston_features.drop(['CRIM'], axis=1)\n",
    "print(df_boston_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Use sklearn.model_selection.train_test_split() to split the features and target values into separate training and test sets. Use 80% of the original data as a training set, and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     MEDV    ZN  INDUS  CHAS    NOX     RM    AGE     DIS   RAD    TAX  \\\n",
      "55   35.4  90.0   1.22   0.0  0.403  7.249   21.9  8.6966   5.0  226.0   \n",
      "273  35.2  20.0   6.96   1.0  0.464  7.691   51.8  4.3665   3.0  223.0   \n",
      "350  22.9  40.0   1.25   0.0  0.429  6.490   44.4  8.7921   1.0  335.0   \n",
      "126  15.7   0.0  25.65   0.0  0.581  5.613   95.6  1.7572   2.0  188.0   \n",
      "11   18.9  12.5   7.87   0.0  0.524  6.009   82.9  6.2267   5.0  311.0   \n",
      "..    ...   ...    ...   ...    ...    ...    ...     ...   ...    ...   \n",
      "407  27.9   0.0  18.10   0.0  0.659  5.608  100.0  1.2852  24.0  666.0   \n",
      "339  19.0   0.0   5.19   0.0  0.515  5.985   45.4  4.8122   5.0  224.0   \n",
      "430  14.5   0.0  18.10   0.0  0.584  6.348   86.1  2.0527  24.0  666.0   \n",
      "182  37.9   0.0   2.46   0.0  0.488  7.155   92.2  2.7006   3.0  193.0   \n",
      "227  31.6   0.0   6.20   0.0  0.504  7.163   79.9  3.2157   8.0  307.0   \n",
      "\n",
      "     PTRATIO       B  LSTAT  \n",
      "55      17.9  395.93   4.81  \n",
      "273     18.6  390.77   6.58  \n",
      "350     19.7  396.90   5.98  \n",
      "126     19.1  359.29  27.26  \n",
      "11      15.2  396.90  13.27  \n",
      "..       ...     ...    ...  \n",
      "407     20.2  332.09  12.13  \n",
      "339     20.2  396.90   9.74  \n",
      "430     20.2   83.45  17.64  \n",
      "182     17.8  394.12   4.82  \n",
      "227     17.4  372.08   6.36  \n",
      "\n",
      "[404 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_boston_features, df_boston_target, test_size=.2)\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create and fit() an sklearn.linear_model.LinearRegression to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0 = [17.76174149]\n",
      "w1 = [-1.89810056e-01  4.85173466e-02 -7.25156399e-02 -6.73905034e-01\n",
      " -9.29427840e+00  2.09214818e-01  1.48984316e-03 -1.04360641e+00\n",
      "  5.78852226e-01 -3.95781002e-03 -2.28791697e-01 -8.15484094e-03\n",
      "  1.01545583e-01]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "x = np.array(x_train)\n",
    "y = np.array(y_train)\n",
    "\n",
    "lm = LinearRegression().fit(x,y)\n",
    "\n",
    "print(f'w0 = {lm.intercept_}')\n",
    "print(f'w1 = {lm.coef_[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Use the predict() method of the model to find the response for each value in the test set, and sklearn.metrics.mean_squared_error(), to find the training and test MSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  39.367922681692114\n",
      "Test:  44.258359558976515\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "    \n",
    "predicted_train = lm.predict(x_train)\n",
    "predicted_test = lm.predict(x_test)\n",
    "mse = mean_squared_error(y_train, predicted_train, squared=True)\n",
    "print('Train: ', mse)\n",
    "mse = mean_squared_error(y_test, predicted_test, squared=True)\n",
    "print('Test: ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. By itself, the MSE doesn’t tell us much. Use the score() method of the model to find the R2 values for the training and test data.\n",
    "\n",
    "##### R2, the coefficient of determination, measures the proportion of variability in the target t that can be explained using the features in X. A value near 1 indicates that most of the variability in the response has been explained by the regression, while a value near 0 indicates that the regression does not explain much of the variability. See Section 3.1.3 of An Introduction to Statistical Learning for details.\n",
    "\n",
    "###### Given the R2 scores, how well did our model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r score:  0.4620036329045377\n",
      "Test r score:  0.4203369336652354\n"
     ]
    }
   ],
   "source": [
    "r_train = lm.score(x_train, y_train)\n",
    "print('Train r score: ', r_train)\n",
    "r_test = lm.score(x_test, y_test)\n",
    "print('Test r score: ', r_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is somewhat accurate.  Our r's are mostly in between 0 and 1 (around .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Let’s see if we can fit the data better with a more flexible model. Scikit-learn can construct polynomial features for us using sklearn.preprocessing.PolynomialFeatures (though note that this includes interaction features as well; you saw in Project 2 that purely polynomial features can easily be constructed using numpy.hstack()).\n",
    "\n",
    "##### Add degree-2 polynomial features, then fit a new linear model. Compare the training and test MSE and R2 scores. Do we seem to be overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  32.767573538174275\n",
      "Testing MSE:  1273.9784845912206\n",
      "Train r score:  -21.674371778352302\n",
      "Test r score:  -15.685622381430647\n"
     ]
    }
   ],
   "source": [
    "t = np.array(y_train['CRIM']).reshape([-1,1])\n",
    "x_reshape_train = np.hstack(((np.array(np.ones_like(x_train['MEDV']))).reshape([-1,1]), np.array(x_train)))\n",
    "for attr in x_train:\n",
    "    xsquared = np.square(np.array(x_train[attr])).reshape([-1,1])\n",
    "    x_reshape_train = np.hstack((x_reshape_train, xsquared))\n",
    "\n",
    "lm = LinearRegression().fit(x_reshape_train, t)  \n",
    "\n",
    "predicted_train = lm.predict(x_reshape_train)\n",
    "mse = mean_squared_error(y_train, predicted_train, squared=True)\n",
    "print('Training MSE: ', mse)\n",
    "\n",
    "t = np.array(y_test['CRIM']).reshape([-1,1])\n",
    "x_reshape_test = np.hstack(((np.array(np.ones_like(x_test['MEDV']))).reshape([-1,1]), np.array(x_test)))\n",
    "for attr in x_test:\n",
    "    xsquared = np.square(np.array(x_test[attr])).reshape([-1,1])\n",
    "    x_reshape_test = np.hstack((x_reshape_test, xsquared))\n",
    "\n",
    "lm = LinearRegression().fit(x_reshape_test, t)  \n",
    "\n",
    "predicted_train = lm.predict(x_reshape_test)\n",
    "mse = mean_squared_error(y_test, predicted_train, squared=True)\n",
    "print('Testing MSE: ', mse)\n",
    "\n",
    "\n",
    "r_train = lm.score(x_reshape_train, y_train)\n",
    "print('Train r score: ', r_train)\n",
    "r_test = lm.score(x_reshape_test, y_test)\n",
    "print('Test r score: ', r_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test MSE seems to have improved as well as the training MSE.  The r score seems to also be closer to 0 meaning that the regression does not explain much of the variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Regularization would allow us to construct a model of intermediate complexity by penalizing large values for the coefficients. Scikit-learn provides this as sklearn.linear_model.Ridge. The parameter alpha corresponds to 𝜆 as shown in the textbook. For now, leave it set to the default value of 1.0, and fit the model to the degree-2 polynomial features. Don’t forget to normalize your features.\n",
    "#### Once again, compare the training and test MSE and R2 scores. Is this model an improvement?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  41.74784384916144\n",
      "Testing MSE:  47.56699214405758\n",
      "Train r score:  0.42947997265391635\n",
      "Test r score:  0.37700292560993187\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "clf = Ridge(alpha=1.0, normalize=True)\n",
    "pm = clf.fit(x_reshape_train, y_train)\n",
    "predicted_train = pm.predict(x_reshape_train)\n",
    "mse = mean_squared_error(y_train, predicted_train, squared=True)\n",
    "print('Training MSE: ', mse)\n",
    "\n",
    "predicted_test = pm.predict(x_reshape_test)\n",
    "mse = mean_squared_error(y_test, predicted_test, squared=True)\n",
    "print('Testing MSE: ', mse)\n",
    "\n",
    "r_train = pm.score(x_reshape_train, y_train)\n",
    "print('Train r score: ', r_train)\n",
    "r_test = pm.score(x_reshape_test, y_test)\n",
    "print('Test r score: ', r_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does not seem to improve anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. We used the default penalty value of 1.0 in the previous experiment, but there’s no reason to believe that this is optimal. Use sklearn.linear_model.RidgeCV to find an optimal value for alpha. How does this compare to experiment (8)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  38.050178880414954\n",
      "Testing MSE:  43.181827227141405\n",
      "Train r score:  0.4800117300952836\n",
      "Test r score:  0.43443655323311625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "clf = RidgeCV(normalize=True)\n",
    "pm = clf.fit(x_reshape_train, y_train)\n",
    "predicted_train = pm.predict(x_reshape_train)\n",
    "mse = mean_squared_error(y_train, predicted_train, squared=True)\n",
    "print('Training MSE: ', mse)\n",
    "\n",
    "predicted_test = pm.predict(x_reshape_test)\n",
    "mse = mean_squared_error(y_test, predicted_test, squared=True)\n",
    "print('Testing MSE: ', mse)\n",
    "\n",
    "r_train = pm.score(x_reshape_train, y_train)\n",
    "print('Train r score: ', r_train)\n",
    "r_test = pm.score(x_reshape_test, y_test)\n",
    "print('Test r score: ', r_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores overall improved. With the r score values slightly improving.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
